# == chatbot_ed_app.py ==
import os
import shutil
import tempfile
import json
import streamlit as st
from cryptography.fernet import Fernet
import pandas as pd
from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_community.document_loaders import PyPDFLoader
from langchain.chains import LLMChain
from langchain.prompts import PromptTemplate
from create_rag import decrypt_folder
from langchain_core.documents import Document
import asyncio
import traceback

# --- CONSTANTES DE CIFRADO ---
fernet_key = st.secrets["encryption"]["key"].encode()
fernet = Fernet(fernet_key)

# --- RUTAS Y CONSTANTES ---
DATA_DIR = "data"
TMP_DIR = tempfile.gettempdir()  # Carpeta temporal del sistema

USERS_CSV_ENC = os.path.join(DATA_DIR, "users.csv.encrypted")
USERS_CSV_TMP = os.path.join(TMP_DIR, "users.csv.tmp")

HIST_ENC = os.path.join(DATA_DIR, "user_profiles.json.encrypted")
HIST_TMP = os.path.join(TMP_DIR, "user_profiles.json.tmp")

VECTORSTORE_ENC = os.path.join(DATA_DIR, ".RAG.encrypted")
VECTORSTORE_DIR = os.path.join(DATA_DIR, ".RAG")

# --- FUNCIONES DE CIFRADO/DESCIFRADO ---

def decrypt_file(src_path: str, dest_path: str):
    token = open(src_path, 'rb').read()
    data = fernet.decrypt(token)
    with open(dest_path, 'wb') as f:
        f.write(data)


def encrypt_file(src_path: str, dest_path: str):
    data = open(src_path, 'rb').read()
    token = fernet.encrypt(data)
    with open(dest_path, 'wb') as f:
        f.write(token)
    os.remove(src_path)

# --- CARGA DE USUARIOS ---
@st.cache_data(ttl=3600)
def cargar_datos_estudiantes():
    if not os.path.exists(USERS_CSV_TMP):
        decrypt_file(USERS_CSV_ENC, USERS_CSV_TMP)
    df = pd.read_csv(USERS_CSV_TMP, dtype=str)
    df['IDCV'] = df['IDCV'].str.strip()
    os.remove(USERS_CSV_TMP)  # Limpieza
    return df

# --- GESTIÃ“N DE HISTORIAL CIFRADO ---
def cargar_o_crear_historiales():
    if os.path.exists(HIST_ENC) and not os.path.exists(HIST_TMP):
        decrypt_file(HIST_ENC, HIST_TMP)
    if not os.path.exists(HIST_TMP):
        with open(HIST_TMP, 'w', encoding='utf-8') as f:
            json.dump({}, f)
    try:
        return json.load(open(HIST_TMP, 'r', encoding='utf-8'))
    except:
        return {}

def guardar_historial(datos: dict):
    with open(HIST_TMP, 'w', encoding='utf-8') as f:
        json.dump(datos, f, ensure_ascii=False, indent=4)
    encrypt_file(HIST_TMP, HIST_ENC)
    os.remove(HIST_TMP)  # Limpieza


@st.cache_resource
def inicializar_vectorstore(api_key: str):
    try:
        asyncio.get_running_loop()
    except RuntimeError:
        asyncio.set_event_loop(asyncio.new_event_loop())

    embeddings = GoogleGenerativeAIEmbeddings(
        model="models/embedding-001",
        google_api_key=api_key
    )

    # Vectorstore FAISS en memoria
    return FAISS.from_texts(["dummy"], embedding=embeddings)  # Puedes cargar documentos reales luego





# --- PLANTILLA PERSONALIZADA ---
prompt_template_str = """
Eres un tutor de programaciÃ³n experto y tu objetivo es personalizar la asistencia basÃ¡ndote en el historial del estudiante para fomentar la innovaciÃ³n y el pensamiento crÃ­tico. No debes dar respuestas directas. Tu mÃ©todo se basa en guiar al estudiante hacia la soluciÃ³n.

**Contexto del Historial del Estudiante:**
{chat_history}

**Documentos de la Asignatura:**
{context}

**Reglas Estrictas de InteracciÃ³n:**
1. Usa el MÃ©todo SocrÃ¡tico: nunca des la respuesta directa. GuÃ­a con preguntas.
2. Adapta la dificultad segÃºn el historial.
3. Fomenta la autoexplicaciÃ³n.
4. Da retroalimentaciÃ³n constructiva y personalizada.
5. Estimula la curiosidad: termina con preguntas abiertas.

**ImplementaciÃ³n en Java o C segÃºn indique el estudiante.**

**Pregunta Actual:**
{question}

**Respuesta del Tutor:**"""

# --- CONFIGURACIÃ“N STREAMLIT ---
st.set_page_config(page_title="Tutor ED App", layout="wide")

# --- LOGIN ---
st.header("ðŸ¤– Tutor de Estructuras de Datos")
df_estudiantes = cargar_datos_estudiantes()

# Obtener parÃ¡metros de la URL
params = st.query_params
idcv_param = params.get("idcv", [None])[0]
nombre_param = params.get("nombre", [None])[0]

# ValidaciÃ³n: solo si vienen por GET y son vÃ¡lidos
if idcv_param and nombre_param:
    user = df_estudiantes[df_estudiantes['IDCV'] == idcv_param.strip()]
    if not user.empty:
        st.session_state.authenticated = True
        st.session_state.user_idcv = idcv_param.strip()
        st.session_state.user_name = user.iloc[0]['Nombre']
    else:
        st.error(f"âŒ Este usuario no tiene permiso para usar el tutor.\n\nPor favor, contacta con el profesor de la asignatura. \nIDCV recibido: {idcv_param}\nNombre recibido: {nombre_param}")
        st.stop()
else:
    st.error(f"âŒ Acceso no autorizado. Faltan credenciales vÃ¡lidas en la URL.\n\nPor favor, contacta con el profesor de la asignatura.")
    st.stop()


# --- CHAT ---
st.title(f"Hola, {st.session_state.user_name}")
user_profiles = cargar_o_crear_historiales()
uid = st.session_state.user_idcv
if uid not in user_profiles:
    user_profiles[uid]={"nombre":st.session_state.user_name,"historial":[]}
history=user_profiles[uid]["historial"]

# Inicializa LLM y vectorstore
api_key=st.secrets["GOOGLE_API_KEY"]
try:
    vectorstore = inicializar_vectorstore(api_key)
except Exception as e:
    st.error("âŒ Error al inicializar el vectorstore:")
    st.code(traceback.format_exc(), language="python")
    st.stop()

llm=ChatGoogleGenerativeAI(model="gemini-2.5-pro",google_api_key=api_key,temperature=0.5)

# Mensajes previos\if "messages" not in st.session_state:
st.session_state.messages=[{"role":"assistant","content":"Â¿Sobre quÃ© tema o estructura de datos tienes dudas hoy?"}]
for m in st.session_state.messages:
    st.chat_message(m["role"]).markdown(m["content"])

# Entrada usuario
if prompt:=st.chat_input("Escribe aquÃ­ tu duda..."):
    st.session_state.messages.append({"role":"user","content":prompt})
    st.chat_message("user").markdown(prompt)
    # Recuperar docs
    docs = vectorstore.similarity_search(prompt, k=5)
    context = "\n\n".join([d.page_content for d in docs])
    # Historial formateado
    last5=history[-5:]
    chat_hist="\n".join([f"- {h['prompt']} => {h['respuesta']}" for h in last5]) or "El estudiante no tiene interacciones previas."
    # Ejecutar LLM
    template=PromptTemplate(template=prompt_template_str, input_variables=["chat_history","context","question"])
    chain=LLMChain(llm=llm,prompt=template)
    resp=chain.invoke({"chat_history":chat_hist,"context":context,"question":prompt}).get("text")
    st.chat_message("assistant").markdown(resp)
    # Guardar
    history.append({"prompt":prompt,"respuesta":resp})
    guardar_historial(user_profiles)
    st.session_state.messages.append({"role":"assistant","content":resp})
