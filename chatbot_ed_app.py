# == chatbot_ed_app.py (Versi√≥n Final v7 - Con control de Reset) ==
import os
import streamlit as st
import pandas as pd
from datetime import datetime
import asyncio
import traceback

# --- LIBRER√çAS DE IA (LangChain y Google) ---
from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.prompts import PromptTemplate

# --- LIBRER√çAS DE FIREBASE ---
import google.oauth2.service_account
from google.cloud import firestore
from cryptography.fernet import Fernet # Necesario para cargar datos

# --- RUTAS Y CONSTANTES ---
TMP_DIR = "tmp"
INDEX_PATH = "faiss_index"
USERS_CSV_PATH_ENC = "data/users.csv.encrypted"

if not os.path.exists(TMP_DIR):
    os.makedirs(TMP_DIR)
USERS_CSV_PATH_TMP = os.path.join(TMP_DIR, "users.csv.tmp")

# --- CONEXI√ìN A FIREBASE ---
@st.cache_resource
def get_firestore_client():
    key_dict = st.secrets["firebase_service_account"]
    creds = google.oauth2.service_account.Credentials.from_service_account_info(key_dict)
    db = firestore.Client(credentials=creds)
    return db

# --- GESTI√ìN DE HISTORIAL ---
def load_active_user_history(db: firestore.Client, user_id: str):
    historial_ref = db.collection('users').document(user_id).collection('historial')
    reset_query = historial_ref.where("role", "==", "system").order_by("timestamp", direction=firestore.Query.DESCENDING).limit(1)
    reset_docs = list(reset_query.stream())
    last_reset_timestamp = reset_docs[0].get('timestamp') if reset_docs else None
    query = historial_ref.where("timestamp", ">", last_reset_timestamp).order_by("timestamp") if last_reset_timestamp else historial_ref.order_by("timestamp")
    return [doc.to_dict() for doc in query.stream()]

def save_message_to_history(db: firestore.Client, user_id: str, message: dict):
    message_to_save = message.copy()
    message_to_save['timestamp'] = firestore.SERVER_TIMESTAMP
    db.collection('users').document(user_id).collection('historial').add(message_to_save)

def reset_user_chat(db: firestore.Client, user_id: str):
    reset_message = {"role": "system", "content": f"--- CHAT RESETEADO POR EL USUARIO ---"}
    save_message_to_history(db, user_id, reset_message)
    st.session_state.messages = []

# --- FUNCIONES AUXILIARES ---
@st.cache_data(ttl=3600)
def cargar_datos_estudiantes():
    fernet_key = st.secrets["encryption"]["key"].encode()
    fernet = Fernet(fernet_key)
    with open(USERS_CSV_PATH_ENC, 'rb') as f_enc:
        token = f_enc.read()
    data = fernet.decrypt(token)
    with open(USERS_CSV_PATH_TMP, 'wb') as f_dec:
        f_dec.write(data)
    df = pd.read_csv(USERS_CSV_PATH_TMP, dtype=str)
    df['IDCV'] = df['IDCV'].str.strip()
    os.remove(USERS_CSV_PATH_TMP)
    return df

@st.cache_resource
def inicializar_vectorstore(api_key: str):
    if not os.path.exists(INDEX_PATH):
        st.error(f"√çndice vectorial '{INDEX_PATH}' no encontrado.")
        return None
    try:
        asyncio.get_running_loop()
    except RuntimeError:
        asyncio.set_event_loop(asyncio.new_event_loop())
    embeddings = GoogleGenerativeAIEmbeddings(model="models/embedding-001", google_api_key=api_key)
    vectorstore = FAISS.load_local(INDEX_PATH, embeddings, allow_dangerous_deserialization=True)
    st.sidebar.success("‚úÖ √çndice RAG cargado.", icon="üìö")
    return vectorstore

# --- L√ìGICA DE RAG EN DOS PASOS ---
file_topic_mapping = {
    # Primeros ficheros de C, punteros y grafos
    "TipoPuntero.pdf": "Descripci√≥n de tipos de punteros en C",
    "Punteros_y_Arrays.pdf": "Relaci√≥n entre punteros y arrays en C y aritm√©tica de punteros",
    "MemoriaDin√°micaParte1.pdf": "Conceptos de memoria din√°mica en C: malloc, calloc, realloc y free",
    "MemoriaDin√°micaParte2.pdf": "Modularidad en C y estructuras din√°micas: colas, pilas y listas enlazadas",
    "Basicos de C.pdf": "Conceptos b√°sicos del lenguaje C: sintaxis, tipos de datos y E/S",
    "Sesion1GrafosR.pdf": "Fundamentos de grafos: definiciones, representaciones y propiedades",
    "Sesion2GrafosR.pdf": "Recorridos en grafos: profundidad, amplitud y algoritmos de recorrido",
    "RepasoJava.pdf": "Revisi√≥n del lenguaje Java: clases, objetos, memoria y registros",
    "Mont√≠culos Binarios (30 oct).pdf": "Mont√≠culos binarios: implementaci√≥n, operaciones y orden del heap",
    # Ficheros de TADs y estructuras en Java/C
    "ListaEnlazadaPriorizada.pdf": "M√≥dulo en C para gesti√≥n de lista enlazada de canales favoritos ordenados: crear, destruir, mostrar e insertar",
    "ADT_Colas.pdf": "Definici√≥n e implementaci√≥n del TAD Cola en Java",
    "ADT_Listas.pdf": "Definici√≥n e implementaci√≥n del TAD Lista en Java",
    "ADT_Pilas.pdf": "Definici√≥n e implementaci√≥n del TAD Pila en Java",
    "ADT_Set.pdf": "Definici√≥n e implementaci√≥n del TAD Conjunto en Java",
    "√Årboles.pdf": "Introducci√≥n a √°rboles gen√©ricos: terminolog√≠a, recorridos y representaci√≥n en Java",
    "BST.pdf": "√Årboles binarios de b√∫squeda: propiedades y operaciones de b√∫squeda, inserci√≥n y eliminaci√≥n",
    "Hash (25 nov) 1 de 2 .pdf": "Fundamentos de tablas hash en Java: funciones de hash, interfaz HashTable<K> y colisiones",
    "Hash (25 nov) 2 de 2 .pdf": "Implementaciones de tablas hash en Java: encadenamiento separado y direccionamiento abierto con rehashing",
    "Implementaci√≥nHash.pdf": "Implementaci√≥n en C de una tabla hash para gesti√≥n de estructuras Persona con resoluci√≥n de colisiones por lista enlazada",
    # Ficheros de mont√≠culos y AVL
    "5√ÅrbolesAVL.pdf": "√Årboles AVL: definici√≥n, propiedades de equilibrio, rotaciones simples y dobles, operaciones de inserci√≥n y eliminaci√≥n, complejidad y detalles de implementaci√≥n en Java",
    "2Izquierdistas.pdf": "Mont√≠culos izquierdistas ponderados: propiedad izquierdista y de orden de mont√≠culo, fusi√≥n eficiente, inserci√≥n, eliminaci√≥n y complejidad, con implementaci√≥n en Java",
}

document_list_description = "\n".join([f"- `{os.path.join('documentos_pdf', key)}`: {value}" for key, value in file_topic_mapping.items()])
source_selection_prompt_template = f"""
Eres un asistente experto en clasificar preguntas... (etc.)"""

def get_relevant_source_file(llm, user_query):
    prompt = PromptTemplate.from_template(source_selection_prompt_template)
    chain = prompt | llm
    response = chain.invoke({"user_query": user_query})
    content = response.content.strip().replace("`", "")
    if content in [os.path.join('documentos_pdf', key) for key in file_topic_mapping.keys()]:
        return content
    return None

# --- INICIO: DEFINICI√ìN DE DOS PLANTILLAS DE PROMPT ---

# Plantilla 1: Se usar√° cuando S√ç encontremos contexto en el RAG
prompt_with_rag_template_str = """
Eres un tutor de programaci√≥n experto y tu objetivo es personalizar la asistencia bas√°ndote en el historial del estudiante para fomentar la innovaci√≥n y el pensamiento cr√≠tico. No debes dar respuestas directas. Tu m√©todo se basa en guiar al estudiante hacia la soluci√≥n.

**Documentos de la Asignatura:**
{context}

**Contexto del Historial del Estudiante:**
{chat_history}

**Reglas Estrictas de Interacci√≥n:**
1. Usa el M√©todo Socr√°tico: nunca des la respuesta directa. Gu√≠a con preguntas.
2. Adapta la dificultad seg√∫n el historial.
3. Fomenta la autoexplicaci√≥n.
4. Da retroalimentaci√≥n constructiva y personalizada.
5. Estimula la curiosidad: termina con preguntas abiertas.

**Implementaci√≥n en Java o C seg√∫n indique el estudiante.**


**Pregunta Actual del Estudiante:**
{question}

**Respuesta del Tutor:**"""

# Plantilla 2: Se usar√° como FALLBACK cuando el RAG no encuentre nada relevante
prompt_without_rag_template_str = """
Eres un tutor de programaci√≥n experto y tu objetivo es personalizar la asistencia bas√°ndote en el historial del estudiante para fomentar la innovaci√≥n y el pensamiento cr√≠tico. No debes dar respuestas directas. Tu m√©todo se basa en guiar al estudiante hacia la soluci√≥n.

**Contexto del Historial del Estudiante:**
{chat_history}

**Reglas Estrictas de Interacci√≥n:**
1. Usa el M√©todo Socr√°tico: nunca des la respuesta directa. Gu√≠a con preguntas.
2. Adapta la dificultad seg√∫n el historial.
3. Fomenta la autoexplicaci√≥n.
4. Da retroalimentaci√≥n constructiva y personalizada.
5. Estimula la curiosidad: termina con preguntas abiertas.

**Implementaci√≥n en Java o C seg√∫n indique el estudiante.**


**Pregunta Actual del Estudiante:**
{question}

**Respuesta del Tutor:**"""

# --- FIN: DEFINICI√ìN DE DOS PLANTILLAS DE PROMPT ---


# --- INICIO DE LA APP ---
st.set_page_config(page_title="Tutor ED App", layout="wide")
st.header("ü§ñ Tutor de Estructuras de Datos (RAG Avanzado)")

# --- LOGIN ---
try:
    df_estudiantes = cargar_datos_estudiantes()
    idcv_value = st.query_params.get("idcv")
    nombre_value = st.query_params.get("nombre")
    if idcv_value and nombre_value:
        user_data = df_estudiantes[df_estudiantes['IDCV'] == str(idcv_value)]
        if not user_data.empty:
            st.session_state.authenticated = True
            st.session_state.user_idcv = idcv_value
            st.session_state.user_name = user_data.iloc[0]['Nombre']
        else:
            st.error(f"‚ùå Usuario no autorizado. IDCV recibido: {idcv_value}")
            st.stop()
    else:
        st.error("‚ùå Acceso no autorizado. Faltan los par√°metros 'idcv' y 'nombre' en la URL.")
        st.stop()
except FileNotFoundError:
    st.error(f"Error cr√≠tico: El fichero de usuarios '{USERS_CSV_PATH_ENC}' no se encontr√≥.")
    st.stop()

# --- INICIALIZACI√ìN DE SERVICIOS ---
db = get_firestore_client()
api_key = st.secrets["GOOGLE_API_KEY"]
vectorstore = inicializar_vectorstore(api_key)
if vectorstore is None:
    st.stop()
llm = ChatGoogleGenerativeAI(model="gemini-2.5-pro", google_api_key=api_key, temperature=0.5)

# --- INICIO DEL CHAT ---
st.title(f"Hola, {st.session_state.user_name}")

# --- Modificaci√≥n para el bot√≥n de reseteo ---
with st.sidebar:
    st.header("Opciones de Chat")
    
    # El bot√≥n se habilita/deshabilita seg√∫n el estado de la sesi√≥n
    if st.button("üóëÔ∏è Resetear Chat", 
                 help="Inicia una nueva conversaci√≥n. Se desactivar√° hasta tu pr√≥ximo mensaje.", 
                 disabled=st.session_state.get("reset_button_disabled", False)):
        reset_user_chat(db, st.session_state.user_idcv)
        st.session_state.reset_button_disabled = True # Desactivamos el bot√≥n inmediatamente
        st.rerun()

# Inicializaci√≥n de variables de estado
if "messages" not in st.session_state:
    st.session_state.messages = load_active_user_history(db, st.session_state.user_idcv)
    if not st.session_state.messages:
        st.session_state.messages.append({"role": "assistant", "content": "¬øSobre qu√© tema o estructura de datos tienes dudas hoy?"})
        
if "esperando_respuesta" not in st.session_state:
    st.session_state.esperando_respuesta = False
    
if "reset_button_disabled" not in st.session_state:
    st.session_state.reset_button_disabled = False

# Mostrar historial
for message in st.session_state.messages:
    if message.get("role") != "system":
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

# --- L√ìGICA PRINCIPAL DE CHAT ---
if prompt := st.chat_input("Escribe aqu√≠ tu duda...", disabled=st.session_state.esperando_respuesta):
    
    # Reactivar el bot√≥n de reseteo si el usuario interact√∫a
    if st.session_state.get("reset_button_disabled", False):
        st.session_state.reset_button_disabled = False

    user_message = {"role": "user", "content": prompt}
    st.session_state.messages.append(user_message)
    save_message_to_history(db, st.session_state.user_idcv, user_message)
    with st.chat_message("user"):
        st.markdown(prompt)
    st.session_state.esperando_respuesta = True
    st.rerun()

if st.session_state.esperando_respuesta and st.session_state.messages[-1]["role"] == "user":
    try:
        with st.spinner("‚è≥ El tutor est√° pensando..."):
            current_prompt = st.session_state.messages[-1]["content"]
            
            # --- L√ìGICA DE RAG ---
            source_file = get_relevant_source_file(llm, current_prompt)
            docs = []
            if source_file:
                candidate_docs = vectorstore.similarity_search(current_prompt, k=20)
                normalized_target_source = source_file.replace("\\", "/")
                filtered_docs = [doc for doc in candidate_docs if doc.metadata.get("source", "").replace("\\", "/") == normalized_target_source]
                docs = filtered_docs[:5]
            else:
                docs = vectorstore.similarity_search(current_prompt, k=5)
            
            # --- L√ìGICA CONDICIONAL DE PROMPT ---
            last5 = st.session_state.messages[-6:-1] if len(st.session_state.messages) > 1 else []
            chat_hist = "\n\n".join([f"- {('Pregunta' if h['role'] == 'user' else 'Respuesta')}: {h['content']}" for h in last5]) or "El estudiante no tiene interacciones previas."

            if docs:
                st.success(f"‚úÖ Se han encontrado {len(docs)} chunks relevantes. Usando modo RAG.")
                context = "\n\n".join([d.page_content for d in docs])
                template = PromptTemplate(template=prompt_with_rag_template_str, input_variables=["chat_history", "context", "question"])
                chain_input = {"chat_history": chat_hist, "context": context, "question": current_prompt}
            else:
                st.warning("‚ö†Ô∏è No se encontraron chunks relevantes. El tutor responder√° desde su conocimiento general.")
                context = "No se encontr√≥ informaci√≥n en los documentos de la asignatura." 
                template = PromptTemplate(template=prompt_without_rag_template_str, input_variables=["chat_history", "question"])
                chain_input = {"chat_history": chat_hist, "question": current_prompt}
            
            with st.expander("üïµÔ∏è‚Äç‚ôÇÔ∏è **Ver Prompt Enviado al LLM**"):
                filled_prompt = template.format_prompt(**chain_input).to_string()
                st.text_area("Prompt Final Completo", filled_prompt, height=400)

            chain = template | llm
            resp_content = chain.invoke(chain_input).content

        with st.chat_message("assistant"):
            st.markdown(resp_content)
        assistant_message = {"role": "assistant", "content": resp_content}
        st.session_state.messages.append(assistant_message)
        save_message_to_history(db, st.session_state.user_idcv, assistant_message)

    except Exception as e:
        error_message = f"‚ùå Lo siento, ocurri√≥ un error al procesar tu pregunta.\n\nDetalle: {str(e)}"
        st.error(error_message)
        st.code(traceback.format_exc(), language="python")
    
    finally:
        st.session_state.esperando_respuesta = False
        st.rerun()