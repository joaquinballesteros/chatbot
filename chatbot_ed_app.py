# == chatbot_ed_app.py ==
import os
import shutil
import tempfile
import json
import streamlit as st
from cryptography.fernet import Fernet
import pandas as pd
from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings
from langchain_chroma import Chroma
from langchain_community.document_loaders import PyPDFLoader
from langchain.chains import LLMChain
from langchain.prompts import PromptTemplate
from create_rag import decrypt_folder
from langchain_core.documents import Document
import asyncio
import traceback

# --- CONSTANTES DE CIFRADO ---
fernet_key = st.secrets["encryption"]["key"].encode()
fernet = Fernet(fernet_key)

# --- RUTAS Y CONSTANTES ---
DATA_DIR = "data"
USERS_CSV_ENC = os.path.join(DATA_DIR, "users.csv.encrypted")
USERS_CSV_TMP = os.path.join(DATA_DIR, "users.csv.tmp")
HIST_ENC = os.path.join(DATA_DIR, "user_profiles.json.encrypted")
HIST_TMP = os.path.join(DATA_DIR, "user_profiles.json.tmp")
VECTORSTORE_ENC = os.path.join(DATA_DIR, ".RAG.encrypted")
VECTORSTORE_DIR = os.path.join(DATA_DIR, ".RAG")

# --- FUNCIONES DE CIFRADO/DESCIFRADO ---

def decrypt_file(src_path: str, dest_path: str):
    token = open(src_path, 'rb').read()
    data = fernet.decrypt(token)
    with open(dest_path, 'wb') as f:
        f.write(data)


def encrypt_file(src_path: str, dest_path: str):
    data = open(src_path, 'rb').read()
    token = fernet.encrypt(data)
    with open(dest_path, 'wb') as f:
        f.write(token)
    os.remove(src_path)

# --- CARGA DE USUARIOS ---
@st.cache_data(ttl=3600)
def cargar_datos_estudiantes():
    if not os.path.exists(USERS_CSV_TMP):
        decrypt_file(USERS_CSV_ENC, USERS_CSV_TMP)
    df = pd.read_csv(USERS_CSV_TMP, dtype=str)
    df['IDCV'] = df['IDCV'].str.strip()
    os.remove(USERS_CSV_TMP)
    return df

# --- GESTIÃ“N DE HISTORIAL CIFRADO ---
def cargar_o_crear_historiales():
    if os.path.exists(HIST_ENC) and not os.path.exists(HIST_TMP):
        decrypt_file(HIST_ENC, HIST_TMP)
    if not os.path.exists(HIST_TMP):
        os.makedirs(DATA_DIR, exist_ok=True)
        with open(HIST_TMP, 'w', encoding='utf-8') as f:
            json.dump({}, f)
    try:
        return json.load(open(HIST_TMP, 'r', encoding='utf-8'))
    except:
        return {}


def guardar_historial(datos: dict):
    with open(HIST_TMP, 'w', encoding='utf-8') as f:
        json.dump(datos, f, ensure_ascii=False, indent=4)
    encrypt_file(HIST_TMP, HIST_ENC)

@st.cache_resource
def inicializar_vectorstore(api_key: str):
    try:
        asyncio.get_running_loop()
    except RuntimeError:
        asyncio.set_event_loop(asyncio.new_event_loop())

    embeddings = GoogleGenerativeAIEmbeddings(model="models/embedding-001", google_api_key=api_key)

    # Chroma en memoria
    return Chroma(embedding_function=embeddings)  # No usar persist_directory



# --- PLANTILLA PERSONALIZADA ---
prompt_template_str = """
Eres un tutor de programaciÃ³n experto y tu objetivo es personalizar la asistencia basÃ¡ndote en el historial del estudiante para fomentar la innovaciÃ³n y el pensamiento crÃ­tico. No debes dar respuestas directas. Tu mÃ©todo se basa en guiar al estudiante hacia la soluciÃ³n.

**Contexto del Historial del Estudiante:**
{chat_history}

**Documentos de la Asignatura:**
{context}

**Reglas Estrictas de InteracciÃ³n:**
1. Usa el MÃ©todo SocrÃ¡tico: nunca des la respuesta directa. GuÃ­a con preguntas.
2. Adapta la dificultad segÃºn el historial.
3. Fomenta la autoexplicaciÃ³n.
4. Da retroalimentaciÃ³n constructiva y personalizada.
5. Estimula la curiosidad: termina con preguntas abiertas.

**ImplementaciÃ³n en Java o C segÃºn indique el estudiante.**

**Pregunta Actual:**
{question}

**Respuesta del Tutor:**"""

# --- CONFIGURACIÃ“N STREAMLIT ---
st.set_page_config(page_title="Tutor ED App", layout="wide")

# --- LOGIN ---
st.header("ðŸ¤– Tutor de Estructuras de Datos")
df_estudiantes = cargar_datos_estudiantes()
if "authenticated" not in st.session_state:
    st.session_state.authenticated = False

if not st.session_state.authenticated:
    with st.form("login_form"):
        codigo = st.text_input("CÃ³digo de Acceso")
        if st.form_submit_button("Acceder"):
            user = df_estudiantes[df_estudiantes['IDCV']==codigo.strip()]
            if not user.empty:
                st.session_state.authenticated=True
                st.session_state.user_idcv=codigo.strip()
                st.session_state.user_name=user.iloc[0]['Nombre']
            else:
                st.error("CÃ³digo no vÃ¡lido.")
    # Si aÃºn no se ha autenticado, detenemos aquÃ­
    if not st.session_state.authenticated:
        st.stop()

# --- CHAT ---
st.title(f"Hola, {st.session_state.user_name}")
user_profiles = cargar_o_crear_historiales()
uid = st.session_state.user_idcv
if uid not in user_profiles:
    user_profiles[uid]={"nombre":st.session_state.user_name,"historial":[]}
history=user_profiles[uid]["historial"]

# Inicializa LLM y vectorstore
api_key=st.secrets["GOOGLE_API_KEY"]
try:
    vectorstore = inicializar_vectorstore(api_key)
except Exception as e:
    st.error("âŒ Error al inicializar el vectorstore:")
    st.code(traceback.format_exc(), language="python")
    st.stop()

llm=ChatGoogleGenerativeAI(model="gemini-2.5-pro",google_api_key=api_key,temperature=0.5)

# Mensajes previos\if "messages" not in st.session_state:
st.session_state.messages=[{"role":"assistant","content":"Â¿Sobre quÃ© tema o estructura de datos tienes dudas hoy?"}]
for m in st.session_state.messages:
    st.chat_message(m["role"]).markdown(m["content"])

# Entrada usuario
if prompt:=st.chat_input("Escribe aquÃ­ tu duda..."):
    st.session_state.messages.append({"role":"user","content":prompt})
    st.chat_message("user").markdown(prompt)
    # Recuperar docs
    retr=vectorstore.as_retriever(search_kwargs={"k":5})
    docs=retr.get_relevant_documents(prompt)
    context="\n\n".join([d.page_content for d in docs])
    # Historial formateado
    last5=history[-5:]
    chat_hist="\n".join([f"- {h['prompt']} => {h['respuesta']}" for h in last5]) or "El estudiante no tiene interacciones previas."
    # Ejecutar LLM
    template=PromptTemplate(template=prompt_template_str, input_variables=["chat_history","context","question"])
    chain=LLMChain(llm=llm,prompt=template)
    resp=chain.invoke({"chat_history":chat_hist,"context":context,"question":prompt}).get("text")
    st.chat_message("assistant").markdown(resp)
    # Guardar
    history.append({"prompt":prompt,"respuesta":resp})
    guardar_historial(user_profiles)
    st.session_state.messages.append({"role":"assistant","content":resp})
