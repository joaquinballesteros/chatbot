import streamlit as st
import os
import nest_asyncio
from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI
from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate # <-- 1. IMPORTAMOS LA CLASE PROMPTTEMPLATE


MODEL_NAME = "gemini-2.5-pro" 
# Aplica el parche para el error de event loop en Streamlit
nest_asyncio.apply()


# --- ConfiguraciÃ³n Inicial y Carga de la API Key ---
# Usamos st.secrets para manejar la API key de forma segura cuando despleguemos.
# Para pruebas locales, puedes comentar esta lÃ­nea y descomentar la siguiente.
google_api_key = st.secrets["GOOGLE_API_KEY"]

# --- CONFIGURACIÃ“N DE LA PÃGINA ---
st.set_page_config(
    page_title="Tutor de Estructuras de Datos",
    page_icon="ðŸ¤–",
    layout="wide"
)

# --- PLANTILLA DE PROMPT PERSONALIZADA ---
# AquÃ­ definimos las instrucciones para nuestro chatbot pedagÃ³gico.
prompt_personalizado_template = """
Eres un tutor de programaciÃ³n experto y tu objetivo principal es fomentar la innovaciÃ³n y el pensamiento crÃ­tico en el estudiante. No debes dar respuestas directas. Tu mÃ©todo se basa en guiar al estudiante hacia la soluciÃ³n. Usa los siguientes documentos de la asignatura para fundamentar tus respuestas.

Documentos disponibles de la asignatura, referencialos por su nombre de archivo y nÃºmero de pÃ¡gina:
{context}

Las implementaciones serÃ¡n en Java o C, asegÃºrate que el estudiante sabe en que lenguaje estÃ¡ interesado en programar la soluciÃ³n.
Sigue estas reglas de forma estricta al interactuar con el estudiante:

1.  **Usa el MÃ©todo SocrÃ¡tico:** Nunca des la respuesta directa. En su lugar, formula preguntas inteligentes que guÃ­en al estudiante a deducir la soluciÃ³n. Si te preguntan "cÃ³mo hacer X", responde con preguntas como "Â¿QuÃ© has intentado hasta ahora?", "Â¿CuÃ¡l crees que serÃ­a el primer paso?".

2.  **Ofrece Pistas Graduales (Scaffolding):** Si el estudiante estÃ¡ atascado, proporciona pistas, empezando por las mÃ¡s generales. Solo si sigue sin avanzar, ofrÃ©cele una pista un poco mÃ¡s concreta.

3.  **Fomenta la AutoexplicaciÃ³n:** Pide constantemente al estudiante que explique su razonamiento con frases como: "Interesante, Â¿puedes explicarme por quÃ© elegiste esta estructura?".

4.  **Da RetroalimentaciÃ³n Constructiva:** Si un cÃ³digo funciona, desafÃ­alo a mejorarlo: "Â¿Crees que podrÃ­as hacerlo mÃ¡s eficiente?". Si es incorrecto, guÃ­a el error: "Â¿QuÃ© pasarÃ­a si la entrada fuera un nÃºmero negativo?".

5.  **Estimula la Curiosidad:** Termina tus respuestas con preguntas abiertas que inviten a explorar mÃ¡s allÃ¡ del problema original.

Pregunta del estudiante:
{question}

Respuesta del Tutor (siguiendo todas las reglas):
"""


# --- Funciones Principales (con cachÃ© para eficiencia) ---

@st.cache_resource
def cargar_documentos(directorio):
    documentos = []
    for filename in os.listdir(directorio):
        if filename.endswith('.pdf'):
            path = os.path.join(directorio, filename)
            loader = PyPDFLoader(path)
            documentos.extend(loader.load())
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=250)
    return text_splitter.split_documents(documentos)

@st.cache_resource
def crear_base_de_datos_vectorial(_textos_divididos, _api_key):
    if not _textos_divididos: return None
    embeddings = GoogleGenerativeAIEmbeddings(model="models/embedding-001", google_api_key=_api_key)
    vectorstore = Chroma.from_documents(documents=_textos_divididos, embedding=embeddings)
    return vectorstore

@st.cache_resource
def crear_cadena_qa(_vectorstore, _api_key):
    """
    MODIFICADO: Esta funciÃ³n ahora usa la plantilla de prompt personalizada.
    """
    if _vectorstore is None: return None
    
    llm = ChatGoogleGenerativeAI(model=MODEL_NAME, google_api_key=_api_key, temperature=0.5)
    
    # 2. CREAMOS UN OBJETO PROMPT CON NUESTRA PLANTILLA
    PROMPT = PromptTemplate(
        template=prompt_personalizado_template, input_variables=["context", "question"]
    )
    
    retriever = _vectorstore.as_retriever(search_kwargs={'k': 5})
    
    # 3. PASAMOS EL PROMPT PERSONALIZADO A LA CADENA
    qa_chain = RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="stuff",
        retriever=retriever,
        return_source_documents=True,
        chain_type_kwargs={"prompt": PROMPT} # <-- AquÃ­ estÃ¡ la magia
    )
    return qa_chain


# --- Interfaz de Usuario con Streamlit ---

st.title("ðŸ¤– Tutor ED - UMA")
st.write("Haz una pregunt, te guiarÃ© para que encuentres la soluciÃ³n.")
st.markdown("---")

doc_directory = "documentos"
if not os.path.exists(doc_directory):
    os.makedirs(doc_directory)
    st.sidebar.info(f"He creado la carpeta '{doc_directory}'. Â¡Sube tus PDFs ahÃ­ y refresca la pÃ¡gina!")

if not google_api_key:
    st.info("Por favor, introduce tu Google API Key en la barra lateral para comenzar.")
    st.stop()

try:
    with st.spinner("Analizando los materiales del curso... Esto solo ocurre una vez."):
        textos = cargar_documentos(doc_directory)
        if not textos:
            st.warning("No he encontrado PDFs en la carpeta 'documentos'. Por favor, sube al menos un archivo y refresca la pÃ¡gina.")
            st.stop()
        vector_db = crear_base_de_datos_vectorial(textos, google_api_key)
        qa_chain = crear_cadena_qa(vector_db, google_api_key)
    st.sidebar.success(f"Â¡Materiales listos! Baso mi conocimiento en {len(textos)} fragmentos de texto.")

except Exception as e:
    st.error(f"Ha ocurrido un error durante la inicializaciÃ³n.")
    st.error(f"Error: {e}")
    st.info("AsegÃºrate de que tu API Key de Google es correcta y tiene permisos. Revisa tambiÃ©n que los archivos PDF no estÃ©n corruptos.")
    st.stop()

# --- LÃ³gica del Chat ---
if "messages" not in st.session_state:
    st.session_state.messages = [{"role": "assistant", "content": "Â¿En quÃ© concepto o problema podemos trabajar hoy?"}]

for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

if prompt := st.chat_input("Escribe aquÃ­ tu duda o cÃ³digo..."):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    with st.chat_message("assistant"):
        with st.spinner("Pensando en una buena pregunta para ti..."):
            if qa_chain:
                response = qa_chain({"query": prompt})
                full_response = response["result"]
                st.markdown(full_response)
                # Opcional: mostrar las fuentes
                with st.expander("Fuentes consultadas en los documentos"):
                    for doc in response["source_documents"]:
                        st.write(f"- {os.path.basename(doc.metadata.get('source', 'N/A'))} (PÃ¡gina ~{doc.metadata.get('page', 'N/A')})")
            else:
                st.error("El chatbot no estÃ¡ listo.")
    
    st.session_state.messages.append({"role": "assistant", "content": full_response})